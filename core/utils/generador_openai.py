import os
import time
import openai
from typing import Optional

# ‚úÖ Configurar la API key aqu√≠, donde se usa
openai.api_key = os.getenv("OPENAI_API_KEY")

def generar_respuesta_con_openai(
    prompt: str,
    contador: int | None = None,
    user_id: str | None = None,
    mensaje_usuario: str | None = None,
    mensaje_original: str | None = None,
    *,
    temperature: float | None = None,   # nuevo: par√°metro oficial en ingl√©s
    max_tokens: int | None = None,      # nuevo: permite override de tokens
    **kwargs,                           # nuevo: tolera kwargs extra (compat futura)
) -> str | None:
    """
    Wrapper estable para ChatCompletion:
    - Acepta 'temperature' y el alias 'temperatura' (compat en espa√±ol).
    - Reintentos con backoff ante fallos transitorios.
    - Si corta por longitud, reintenta con m√°s cupo de tokens.
    """

    # Alias compatible para llamadas antiguas: 'temperatura='
    if temperature is None:
        temperature = kwargs.pop("temperatura", None)

    # Defaults si no pasan nada
    if temperature is None:
        temperature = 0.2  # antes usabas 0; pod√©s volver a 0 si prefer√≠s ese estilo
    MAX_TOKENS_PRIMARY = max_tokens or 200
    MAX_TOKENS_FALLBACK = max(400, MAX_TOKENS_PRIMARY * 2)  # mantiene tu 400 por defecto

    # Pod√©s sobreescribir el modelo por env si quer√©s
    modelo = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")

    TIMEOUT_SECONDS = 12
    RETRIES = 2  # cantidad de reintentos ante fallo (adem√°s del intento inicial)

    for intento in range(RETRIES + 1):
        try:
            respuesta = openai.ChatCompletion.create(
                model=modelo,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=MAX_TOKENS_PRIMARY,
                n=1,
                request_timeout=TIMEOUT_SECONDS,
            )

            choice = respuesta.choices[0]
            # robusto: soporte atributo/clave indistintamente
            try:
                contenido = (choice.message["content"] if isinstance(choice.message, dict) else choice.message.content).strip()
            except Exception:
                # fallback s√∫per conservador
                contenido = (getattr(choice, "text", "") or "").strip()

            # ¬øcort√≥ por longitud? reintentamos con m√°s cupo
            finish_reason = getattr(choice, "finish_reason", None)
            if not finish_reason:
                try:
                    finish_reason = choice.get("finish_reason")
                except Exception:
                    pass

            if finish_reason == "length":
                try:
                    respuesta2 = openai.ChatCompletion.create(
                        model=modelo,
                        messages=[{"role": "user", "content": prompt}],
                        temperature=temperature,
                        max_tokens=MAX_TOKENS_FALLBACK,
                        n=1,
                        request_timeout=TIMEOUT_SECONDS,
                    )
                    choice2 = respuesta2.choices[0]
                    try:
                        contenido = (choice2.message["content"] if isinstance(choice2.message, dict) else choice2.message.content).strip()
                    except Exception:
                        contenido = (getattr(choice2, "text", "") or "").strip()
                except Exception as e_len:
                    print(f"‚ö†Ô∏è Reintento por length fall√≥: {e_len}")

            # Log opcional
            try:
                print(f"üß† OpenAI respondi√≥ (contador={contador}) ‚Üí {contenido}")
            except Exception:
                pass

            return contenido or None

        except Exception as e:
            print(f"‚ö†Ô∏è OpenAI intento {intento + 1} fall√≥: {e}")
            time.sleep(0.5 * (2 ** intento))  # backoff exponencial suave

    print("[ERROR OPENAI] agotados intentos")
    return None
